{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOqGqcgPVBA0QZ8Je5HTX68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/ia_et_shs/blob/main/5_postagging_with_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu datasets transformers"
      ],
      "metadata": {
        "id": "YuTZwzNq4InN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "PUrCiGBdhAO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqNXxLdKyASB"
      },
      "outputs": [],
      "source": [
        "model_name = \"bowphs/GreTa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "encoder_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Vérifiez le modèle\n",
        "print(encoder_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --remote-name-all https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-5787{/ud-treebanks-v2.15.tgz,/ud-documentation-v2.15.tgz,/ud-tools-v2.15.tgz}"
      ],
      "metadata": {
        "id": "BHO7IzU712OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf ud-treebanks-v2.15.tgz"
      ],
      "metadata": {
        "id": "nqN5iiYS2nZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from conllu import parse\n",
        "\n",
        "def load_conllu(filename):\n",
        "    \"\"\"\n",
        "    Charge un fichier .conllu et extrait les tokens et leurs tags.\n",
        "    Retourne deux listes :\n",
        "    - sentences : liste des listes de mots (tokens)\n",
        "    - taggings : liste des listes de tags (UPOS)\n",
        "    \"\"\"\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as fp:\n",
        "        data = parse(fp.read())\n",
        "\n",
        "    sentences = []\n",
        "    taggings = []\n",
        "\n",
        "    for sentence in data:\n",
        "        tokens = [token['form'] for token in sentence]\n",
        "        tags = [token['upostag'] for token in sentence]\n",
        "\n",
        "        # Ajouter uniquement les phrases valides (tokens et tags alignés)\n",
        "        if tokens and tags and len(tokens) == len(tags):\n",
        "            sentences.append(tokens)\n",
        "            taggings.append(tags)\n",
        "\n",
        "    return sentences, taggings"
      ],
      "metadata": {
        "id": "yraweLqdS93x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/ud-treebanks-v2.15/UD_Ancient_Greek-PROIEL/\"\n",
        "train_sentences, train_tags = load_conllu(base_path + \"grc_proiel-ud-train.conllu\")\n",
        "dev_sentences, dev_tags = load_conllu(base_path + \"grc_proiel-ud-dev.conllu\")\n",
        "test_sentences, test_tags = load_conllu(base_path + \"grc_proiel-ud-test.conllu\")\n",
        "\n",
        "# Exemple d'affichage\n",
        "print(\"Exemple de phrase :\", train_sentences[0])\n",
        "print(\"Tags correspondants :\", train_tags[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BozIXzKe6v45",
        "outputId": "d2b4ba75-3143-40c0-888b-01bb4ecc6a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemple de phrase : ['Ἡροδότου', 'Ἁλικαρνησσέος', 'ἱστορίης', 'ἀπόδεξις', 'ἥδε', 'ὡς', 'μήτε', 'τὰ', 'γενόμενα', 'ἐξ', 'ἀνθρώπων', 'τῷ', 'χρόνῳ', 'ἐξίτηλα', 'γένηται', 'μήτε', 'ἔργα', 'μεγάλα', 'τε', 'καὶ', 'θωμαστά', 'τὰ', 'μὲν', 'Ἕλλησι', 'τὰ', 'δὲ', 'βαρβάροισι', 'ἀποδεχθέντα', 'ἀκλεᾶ', 'γένηται', 'τά', 'τε', 'ἄλλα', 'καὶ', 'δι’', 'ἣν', 'αἰτίην', 'ἐπολέμησαν', 'ἀλλήλοισι']\n",
            "Tags correspondants : ['PROPN', 'NOUN', 'NOUN', 'NOUN', 'PRON', 'SCONJ', 'CCONJ', 'DET', 'VERB', 'ADP', 'NOUN', 'DET', 'NOUN', 'ADJ', 'VERB', 'CCONJ', 'NOUN', 'ADJ', 'CCONJ', 'CCONJ', 'ADJ', 'PRON', 'ADV', 'NOUN', 'PRON', 'ADV', 'NOUN', 'VERB', 'ADJ', 'VERB', 'DET', 'CCONJ', 'PRON', 'CCONJ', 'ADP', 'PRON', 'NOUN', 'VERB', 'PRON']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(sentences, tags, tokenizer):\n",
        "    \"\"\"\n",
        "    Prépare les données pour l'entraînement de l'encodeur.\n",
        "    \"\"\"\n",
        "    tag2id = {tag: idx for idx, tag in enumerate(set(tag for tag_seq in tags for tag in tag_seq))}\n",
        "    id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "    max_length = 128  # Limite de longueur\n",
        "\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "\n",
        "    for tokens, tag_seq in zip(sentences, tags):\n",
        "        # Tokenisation des tokens\n",
        "        tokenized = tokenizer(tokens, is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "        # Alignement des tags\n",
        "        word_ids = tokenized.word_ids()  # Récupérer les indices de mots après tokenisation\n",
        "        aligned_tags = [-100 if word_id is None else tag2id[tag_seq[word_id]] for word_id in word_ids]\n",
        "\n",
        "        inputs.append(tokenized)\n",
        "        outputs.append(aligned_tags)\n",
        "\n",
        "    return inputs, outputs, tag2id, id2tag"
      ],
      "metadata": {
        "id": "SnX5LRZ8MihE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    encoder_model.resize_token_embeddings(len(tokenizer))  # Ajuster les embeddings du modèle"
      ],
      "metadata": {
        "id": "AMQc0loqiItV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, train_outputs, tag2id, id2tag = prepare_data(train_sentences, train_tags, tokenizer)\n",
        "dev_inputs, dev_outputs, _, _ = prepare_data(dev_sentences, dev_tags, tokenizer)"
      ],
      "metadata": {
        "id": "Cvw25HumMmwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "class PosDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = torch.tensor(self.inputs[idx][\"input_ids\"])\n",
        "        attention_mask = torch.tensor(self.inputs[idx][\"attention_mask\"])\n",
        "        labels = torch.tensor(self.outputs[idx])\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
      ],
      "metadata": {
        "id": "-_QngZ7fMtv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PosDataset(train_inputs, train_outputs)\n",
        "dev_dataset = PosDataset(dev_inputs, dev_outputs)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "mQz33FI7ico4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batches = 0\n",
        "total_examples = 0\n",
        "\n",
        "for batch in dev_dataloader:\n",
        "    total_batches += 1\n",
        "    total_examples += batch[\"input_ids\"].size(0)\n",
        "\n",
        "print(f\"Nombre de batchs : {total_batches}\")\n",
        "print(f\"Nombre total d'exemples dans dev_dataloader : {total_examples}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoTWPpzIxeZU",
        "outputId": "b858edbb-a506-4577-f952-3026edfff7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de batchs : 32\n",
            "Nombre total d'exemples dans dev_dataloader : 1019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PosTagger(nn.Module):\n",
        "    def __init__(self, encoder, hidden_dim, num_labels):\n",
        "        super(PosTagger, self).__init__()\n",
        "        self.encoder = encoder.encoder  # Utiliser uniquement l'encodeur de T5\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Passer uniquement par l'encodeur\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = encoder_outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
        "        logits = self.classifier(embeddings)  # [batch_size, seq_len, num_labels]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "kc6xztECdn0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = encoder_model.config.hidden_size\n",
        "num_labels = len(tag2id)\n",
        "pos_tagger = PosTagger(encoder_model, hidden_dim, num_labels)"
      ],
      "metadata": {
        "id": "wyVYXYalizh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pos_tagger.to(device)\n",
        "\n",
        "# Optimiseur et fonction de perte\n",
        "optimizer = AdamW(pos_tagger.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "num_epochs=3\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_training_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Dans la boucle d'entraînement\n",
        "for epoch in range(num_epochs):\n",
        "    pos_tagger.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = pos_tagger(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(logits.view(-1, num_labels), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Met à jour le learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Époque {epoch + 1}/{num_epochs}, Perte moyenne :\", total_loss / len(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szINLpUsdvEn",
        "outputId": "2f103fa0-159c-4ff7-ca73-0c92c5ab22fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Époque 1/3, Perte moyenne : 1.4974161515844628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer sur dev_data\n",
        "pos_tagger.eval()\n",
        "total_accuracy = 0\n",
        "total_examples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dev_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        logits = pos_tagger(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Calculer l'accuracy\n",
        "        for pred, label in zip(predictions, labels):\n",
        "            mask = label != -100\n",
        "            total_accuracy += (pred[mask] == label[mask]).sum().item()\n",
        "            total_examples += mask.sum().item()\n",
        "\n",
        "accuracy = total_accuracy / total_examples\n",
        "print(len(predictions))\n",
        "print(f\"Accuracy sur dev_data : {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "BQpN_2P0d0EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_sentence(sentence, model, tokenizer, id2tag):\n",
        "    \"\"\"\n",
        "    Interroge le modèle sur une phrase donnée pour obtenir les tags PoS.\n",
        "\n",
        "    Args:\n",
        "    - sentence (str): La phrase à taguer.\n",
        "    - model (nn.Module): Le modèle PoS tagger.\n",
        "    - tokenizer (AutoTokenizer): Le tokenizer associé.\n",
        "    - id2tag (dict): Mapping des IDs des tags vers leurs étiquettes textuelles.\n",
        "\n",
        "    Returns:\n",
        "    - tokens (list): Liste des tokens de la phrase.\n",
        "    - predicted_tags (list): Liste des tags prédits pour chaque token.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Prédictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]\n",
        "\n",
        "    # Décoder les tokens et leurs tags correspondants\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=True)\n",
        "    predicted_ids = predictions[0].cpu().numpy()\n",
        "\n",
        "    # Associer les tokens aux tags prédits\n",
        "    predicted_tags = [id2tag[tag_id] for tag_id in predicted_ids if tag_id in id2tag]\n",
        "\n",
        "    # Filtrer pour correspondre uniquement aux tokens d'origine (éviter les sous-tokens)\n",
        "    word_ids = inputs.word_ids()  # Correspondance entre les indices de mots et les tokens\n",
        "    tokens_filtered = []\n",
        "    tags_filtered = []\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        if word_id is not None and (i == 0 or word_id != word_ids[i - 1]):  # Prendre uniquement le premier token d'un mot\n",
        "            tokens_filtered.append(tokens[i])\n",
        "            tags_filtered.append(predicted_tags[i])\n",
        "\n",
        "    return tokens_filtered, tags_filtered\n",
        "\n",
        "\n",
        "sentence = \"Ἄνδρα μοι ἔννεπε, Μοῦσα, πολύτροπον, ὃς μάλα πολλὰ πλάγχθη.\"\n",
        "tokens, tags = tag_sentence(sentence, pos_tagger, tokenizer, id2tag)\n",
        "\n",
        "# Résultat\n",
        "print(\"Phrase :\", sentence)\n",
        "print(\"Tokens :\", tokens)\n",
        "print(\"Tags prédits :\", tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pz2MuEmUk1t",
        "outputId": "d8b0aa9f-e2e3-41ce-9d79-5bd214ea7db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase : Ἄνδρα μοι ἔννεπε, Μοῦσα, πολύτροπον, ὃς μάλα πολλὰ πλάγχθη.\n",
            "Tokens : ['▁ἄνδρα', '▁μοι', '▁ἔννεπ', ',', '▁μοῦσα', ',', '▁πολύτροπο', ',', '▁ὃ', '▁μάλα', '▁πολλὰ', '▁πλά', '.']\n",
            "Tags prédits : ['NOUN', 'PRON', 'VERB', 'ADP', 'NOUN', 'ADP', 'ADJ', 'ADP', 'PRON', 'ADV', 'ADJ', 'VERB', 'VERB']\n"
          ]
        }
      ]
    }
  ]
}